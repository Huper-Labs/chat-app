version: '3.8'

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      # Core Configuration
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - WEBUI_NAME=${WEBUI_NAME}
      - WEBUI_URL=${WEBUI_URL}
      - DATABASE_URL=${DATABASE_URL}
      - ENV=${ENV}
      
      # LiteLLM Integration
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      
      # Security & Authentication
      - ENABLE_SIGNUP=${ENABLE_SIGNUP}
      - ENABLE_LOGIN_FORM=${ENABLE_LOGIN_FORM}
      - DEFAULT_USER_ROLE=${DEFAULT_USER_ROLE}
      - ENABLE_ADMIN_EXPORT=${ENABLE_ADMIN_EXPORT}
      - ENABLE_ADMIN_CHAT_ACCESS=${ENABLE_ADMIN_CHAT_ACCESS}
      - SAFE_MODE=${SAFE_MODE}
      
      # Features
      - ENABLE_RAG_WEB_SEARCH=${ENABLE_RAG_WEB_SEARCH}
      - ENABLE_WEB_SEARCH=${ENABLE_WEB_SEARCH}
      - ENABLE_IMAGE_GENERATION=${ENABLE_IMAGE_GENERATION}
      - ENABLE_CODE_EXECUTION=${ENABLE_CODE_EXECUTION}
      - ENABLE_CHANNELS=${ENABLE_CHANNELS}
      - ENABLE_COMMUNITY_SHARING=${ENABLE_COMMUNITY_SHARING}
      - ENABLE_MESSAGE_RATING=${ENABLE_MESSAGE_RATING}
      - ENABLE_EVALUATION_ARENA_MODELS=${ENABLE_EVALUATION_ARENA_MODELS}
      - ENABLE_TAGS_GENERATION=${ENABLE_TAGS_GENERATION}
      - ENABLE_AUTOCOMPLETE_GENERATION=${ENABLE_AUTOCOMPLETE_GENERATION}
      
      # User Tracking
      - ENABLE_FORWARD_USER_INFO_HEADERS=${ENABLE_FORWARD_USER_INFO_HEADERS}
      
      # RAG Configuration
      - RAG_EMBEDDING_ENGINE=${RAG_EMBEDDING_ENGINE}
      - RAG_RERANKING_MODEL=${RAG_RERANKING_MODEL}
      - RAG_EMBEDDING_MODEL=${RAG_EMBEDDING_MODEL}
      - RAG_EMBEDDING_MODEL_AUTO_UPDATE=${RAG_EMBEDDING_MODEL_AUTO_UPDATE}
      - RAG_RERANKING_MODEL_AUTO_UPDATE=${RAG_RERANKING_MODEL_AUTO_UPDATE}
      - ENABLE_RAG_LOCAL_WEB_FETCH=${ENABLE_RAG_LOCAL_WEB_FETCH}
      - ENABLE_RAG_WEB_LOADER_SSL_VERIFICATION=${ENABLE_RAG_WEB_LOADER_SSL_VERIFICATION}
      
      # Search Configuration
      - WEB_SEARCH_ENGINE=${WEB_SEARCH_ENGINE}
      - WEB_SEARCH_RESULT_COUNT=${WEB_SEARCH_RESULT_COUNT}
      - WEB_SEARCH_CONCURRENT_REQUESTS=${WEB_SEARCH_CONCURRENT_REQUESTS}
      - SEARXNG_QUERY_URL=${SEARXNG_QUERY_URL:-}
      
      # Performance
      - THREAD_POOL_SIZE=${THREAD_POOL_SIZE}
      - DATABASE_POOL_SIZE=${DATABASE_POOL_SIZE}
      - DATABASE_POOL_TIMEOUT=${DATABASE_POOL_TIMEOUT}
      
      # Websocket Support
      - ENABLE_WEBSOCKET_SUPPORT=${ENABLE_WEBSOCKET_SUPPORT}
      - WEBSOCKET_MANAGER=${WEBSOCKET_MANAGER:-}
      - WEBSOCKET_REDIS_URL=${WEBSOCKET_REDIS_URL:-}
      
      # Redis (for multi-node deployments)
      - REDIS_URL=${REDIS_URL:-}
      
      # Audio/STT Configuration
      - AUDIO_STT_ENGINE=${AUDIO_STT_ENGINE:-}
      - AUDIO_STT_MODEL=${AUDIO_STT_MODEL:-}
      - WHISPER_MODEL=${WHISPER_MODEL:-}
      - WHISPER_MODEL_DIR=${WHISPER_MODEL_DIR:-}
      
      # Code Execution
      - CODE_EXECUTION_ENGINE=${CODE_EXECUTION_ENGINE}
      - CODE_EXECUTION_JUPYTER_URL=${CODE_EXECUTION_JUPYTER_URL:-}
      
      # Logging
      - GLOBAL_LOG_LEVEL=${GLOBAL_LOG_LEVEL}
      
      # GPU Support (uncomment if using NVIDIA GPU)
      # - USE_CUDA_DOCKER=${USE_CUDA_DOCKER}
    volumes:
      - ./volumes/open-webui:/app/backend/data
      - ./config/open-webui:/app/backend/config
      # Model cache volumes
      - ./volumes/cache/embedding/models:/app/backend/data/cache/embedding/models
      - ./volumes/cache/whisper/models:/app/backend/data/cache/whisper/models
    depends_on:
      postgres:
        condition: service_healthy
      litellm:
        condition: service_started
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  litellm:
    image: ghcr.io/berriai/litellm-database:main-stable
    container_name: litellm
    ports:
      - "4000:4000"
    environment:
      # Core Settings
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - LITELLM_SALT_KEY=${LITELLM_SALT_KEY}
      - DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/litellm
      - STORE_MODEL_IN_DB=${STORE_MODEL_IN_DB}
      
      # Redis Cache Settings
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      - REDIS_CONNECTION_POOL_TIMEOUT=${REDIS_CONNECTION_POOL_TIMEOUT}
      - REDIS_SOCKET_TIMEOUT=${REDIS_SOCKET_TIMEOUT}
      
      # Proxy Settings
      - PROXY_BASE_URL=${PROXY_BASE_URL}
      - PROXY_BATCH_WRITE_AT=${PROXY_BATCH_WRITE_AT}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT}
      - LITELLM_LOG=${LITELLM_LOG}
      - SET_VERBOSE=${SET_VERBOSE}
      
      # UI Settings
      - UI_USERNAME=${UI_USERNAME}
      - UI_PASSWORD=${UI_PASSWORD}
      
      # Admin Settings
      - DISABLE_ADMIN_UI=${DISABLE_ADMIN_UI}
      - DISABLE_ADMIN_ENDPOINTS=${DISABLE_ADMIN_ENDPOINTS}
      - DISABLE_LLM_ENDPOINTS=${DISABLE_LLM_ENDPOINTS}
      
      # LLM Provider API Keys (add as needed)
      - OPENAI_API_KEY=${PROVIDER_OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${PROVIDER_ANTHROPIC_API_KEY:-}
      - GEMINI_API_KEY=${PROVIDER_GEMINI_API_KEY:-}
      - COHERE_API_KEY=${PROVIDER_COHERE_API_KEY:-}
      - MISTRAL_API_KEY=${PROVIDER_MISTRAL_API_KEY:-}
      - PERPLEXITY_API_KEY=${PROVIDER_PERPLEXITY_API_KEY:-}
      - XAI_API_KEY=${PROVIDER_XAI_API_KEY:-}
      - HUGGINGFACE_API_KEY=${PROVIDER_HUGGINGFACE_API_KEY:-}
      - REPLICATE_API_KEY=${PROVIDER_REPLICATE_API_KEY:-}
      - VOYAGE_API_KEY=${PROVIDER_VOYAGE_API_KEY:-}
      - DATABRICKS_API_KEY=${PROVIDER_DATABRICKS_API_KEY:-}
      - NVIDIA_NIM_API_KEY=${PROVIDER_NVIDIA_NIM_API_KEY:-}
      - CEREBRAS_API_KEY=${PROVIDER_CEREBRAS_API_KEY:-}
      - AI21_API_KEY=${PROVIDER_AI21_API_KEY:-}
      - NEBIUS_API_KEY=${PROVIDER_NEBIUS_API_KEY:-}
      - VOLCENGINE_API_KEY=${PROVIDER_VOLCENGINE_API_KEY:-}
    volumes:
      - ./config/litellm/config.yaml:/app/config.yaml
      - ./volumes/litellm:/app/data
    command: ["--config", "/app/config.yaml", "--port", "4000", "--detailed_debug"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
    volumes:
      - ./volumes/postgres:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  redis:
    image: redis:7-alpine
    container_name: redis
    volumes:
      - ./volumes/redis:/data
    ports:
      - "6379:6379"
    command: redis-server --save 30 1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Optional: Automatic updates with Watchtower
  # watchtower:
  #   image: containrrr/watchtower
  #   container_name: watchtower
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock
  #   command: --interval 300 open-webui litellm
  #   restart: unless-stopped
  #   depends_on:
  #     - open-webui
  #     - litellm

networks:
  default:
    name: open-webui-network
    driver: bridge